{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNXB4zzudkuVY5AI4aAQs71",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nvinogradskaya/Ant/blob/main/Untitled4-3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tensorflow"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bFLyN74Uk3B9",
        "outputId": "1dc924c4-c1fb-43f7-a9ea-c43d06a8a3b2"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting tensorflow\n",
            "  Downloading tensorflow-2.19.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.1 kB)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.4.0)\n",
            "Collecting astunparse>=1.6.0 (from tensorflow)\n",
            "  Downloading astunparse-1.6.3-py2.py3-none-any.whl.metadata (4.4 kB)\n",
            "Collecting flatbuffers>=24.3.25 (from tensorflow)\n",
            "  Downloading flatbuffers-25.2.10-py2.py3-none-any.whl.metadata (875 bytes)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.6.0)\n",
            "Collecting google-pasta>=0.1.1 (from tensorflow)\n",
            "  Downloading google_pasta-0.2.0-py3-none-any.whl.metadata (814 bytes)\n",
            "Collecting libclang>=13.0.0 (from tensorflow)\n",
            "  Downloading libclang-18.1.1-py2.py3-none-manylinux2010_x86_64.whl.metadata (5.2 kB)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.4.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from tensorflow) (24.2)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (5.29.4)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (2.32.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from tensorflow) (75.1.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.17.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (2.5.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (4.12.2)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.17.2)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.71.0)\n",
            "Collecting tensorboard~=2.19.0 (from tensorflow)\n",
            "  Downloading tensorboard-2.19.0-py3-none-any.whl.metadata (1.8 kB)\n",
            "Requirement already satisfied: keras>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.8.0)\n",
            "Requirement already satisfied: numpy<2.2.0,>=1.26.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (2.0.2)\n",
            "Requirement already satisfied: h5py>=3.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.13.0)\n",
            "Requirement already satisfied: ml-dtypes<1.0.0,>=0.5.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.5.1)\n",
            "Collecting tensorflow-io-gcs-filesystem>=0.23.1 (from tensorflow)\n",
            "  Downloading tensorflow_io_gcs_filesystem-0.37.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (14 kB)\n",
            "Collecting wheel<1.0,>=0.23.0 (from astunparse>=1.6.0->tensorflow)\n",
            "  Downloading wheel-0.45.1-py3-none-any.whl.metadata (2.3 kB)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (13.9.4)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (0.0.8)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (0.14.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (2025.1.31)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/lib/python3/dist-packages (from tensorboard~=2.19.0->tensorflow) (3.3.6)\n",
            "Collecting tensorboard-data-server<0.8.0,>=0.7.0 (from tensorboard~=2.19.0->tensorflow)\n",
            "  Downloading tensorboard_data_server-0.7.2-py3-none-manylinux_2_31_x86_64.whl.metadata (1.1 kB)\n",
            "Collecting werkzeug>=1.0.1 (from tensorboard~=2.19.0->tensorflow)\n",
            "  Downloading werkzeug-3.1.3-py3-none-any.whl.metadata (3.7 kB)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.11/dist-packages (from werkzeug>=1.0.1->tensorboard~=2.19.0->tensorflow) (3.0.2)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras>=3.5.0->tensorflow) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras>=3.5.0->tensorflow) (2.19.1)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow) (0.1.2)\n",
            "Downloading tensorflow-2.19.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (644.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m644.9/644.9 MB\u001b[0m \u001b[31m595.8 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading astunparse-1.6.3-py2.py3-none-any.whl (12 kB)\n",
            "Downloading flatbuffers-25.2.10-py2.py3-none-any.whl (30 kB)\n",
            "Downloading google_pasta-0.2.0-py3-none-any.whl (57 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.5/57.5 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading libclang-18.1.1-py2.py3-none-manylinux2010_x86_64.whl (24.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.5/24.5 MB\u001b[0m \u001b[31m49.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tensorboard-2.19.0-py3-none-any.whl (5.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.5/5.5 MB\u001b[0m \u001b[31m93.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tensorflow_io_gcs_filesystem-0.37.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.1/5.1 MB\u001b[0m \u001b[31m79.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tensorboard_data_server-0.7.2-py3-none-manylinux_2_31_x86_64.whl (6.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.6/6.6 MB\u001b[0m \u001b[31m110.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading werkzeug-3.1.3-py3-none-any.whl (224 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m224.5/224.5 kB\u001b[0m \u001b[31m14.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading wheel-0.45.1-py3-none-any.whl (72 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m72.5/72.5 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: libclang, flatbuffers, wheel, werkzeug, tensorflow-io-gcs-filesystem, tensorboard-data-server, google-pasta, tensorboard, astunparse, tensorflow\n",
            "Successfully installed astunparse-1.6.3 flatbuffers-25.2.10 google-pasta-0.2.0 libclang-18.1.1 tensorboard-2.19.0 tensorboard-data-server-0.7.2 tensorflow-2.19.0 tensorflow-io-gcs-filesystem-0.37.1 werkzeug-3.1.3 wheel-0.45.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "sGju9Cf9VMjy"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from joblib import Parallel, delayed\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import LSTM, Dense, Dropout, Input, Embedding, Lambda\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from google.colab import drive\n",
        "from tensorflow.keras.layers import Input, LSTM, Dense, Dropout, Embedding, Lambda, Concatenate, RepeatVector"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "drive.mount('/content/drive')\n",
        "\n",
        "DATA_PATH = \"/content/drive/My Drive/Colab Notebooks/Data/\"\n",
        "SEQ_LENGTH = 10\n",
        "EMBEDDING_DIM = 16\n",
        "BATCH_SIZE = 64\n",
        "EPOCHS = 5"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NNdNOzctVbEE",
        "outputId": "59151a97-0098-42db-845d-4c247493991b"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Блок 2: Определение ContrastiveLoss\n",
        "class ContrastiveLoss(tf.keras.losses.Loss):\n",
        "    def __init__(self, margin=1.0):\n",
        "        super().__init__()\n",
        "        self.margin = margin\n",
        "\n",
        "    def call(self, y_true, y_pred):\n",
        "        y_true = tf.cast(y_true, tf.float32)\n",
        "        anchor, positive = y_pred[:,0], y_pred[:,1]\n",
        "        distances = tf.reduce_sum(tf.square(anchor - positive), axis=-1)\n",
        "        loss = y_true * distances + (1 - y_true) * tf.maximum(self.margin - distances, 0)\n",
        "        return tf.reduce_mean(loss)"
      ],
      "metadata": {
        "id": "OUVR9802gnTy"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_and_preprocess_data(data_path, max_users=10):\n",
        "    data = []\n",
        "    user_dirs = sorted(os.listdir(data_path))[:max_users]\n",
        "\n",
        "    for user in user_dirs:\n",
        "        traj_dir = os.path.join(data_path, user, 'Trajectory')\n",
        "        traj_files = [f for f in os.listdir(traj_dir) if f.endswith('.plt')]\n",
        "\n",
        "        for traj_file in traj_files:\n",
        "            df = pd.read_csv(\n",
        "                os.path.join(traj_dir, traj_file),\n",
        "                skiprows=6,\n",
        "                header=None,\n",
        "                usecols=[0, 1, 3, 5, 6],\n",
        "                names=['lat', 'lon', 'alt', 'date', 'time']\n",
        "            )\n",
        "            df['user'] = user\n",
        "            data.append(df)\n",
        "\n",
        "    df = pd.concat(data, ignore_index=True)\n",
        "    df['datetime'] = pd.to_datetime(df['date'] + ' ' + df['time'])\n",
        "    df.sort_values(by=['user', 'datetime'], inplace=True)\n",
        "\n",
        "    # фильтрация и нормализация\n",
        "    df = df[(df['lat'] != 0) & (df['lon'] != 0)].ffill()\n",
        "    scaler = MinMaxScaler()\n",
        "    df[['lat', 'lon', 'alt']] = scaler.fit_transform(df[['lat', 'lon', 'alt']])\n",
        "\n",
        "    # временные\n",
        "    df['hour_sin'] = np.sin(2 * np.pi * df['datetime'].dt.hour / 24)\n",
        "    df['hour_cos'] = np.cos(2 * np.pi * df['datetime'].dt.hour / 24)\n",
        "    df['day_sin'] = np.sin(2 * np.pi * df['datetime'].dt.dayofweek / 7)\n",
        "    df['day_cos'] = np.cos(2 * np.pi * df['datetime'].dt.dayofweek / 7)\n",
        "\n",
        "    # персонализированные\n",
        "    user_ids = {user: idx for idx, user in enumerate(df['user'].unique())}\n",
        "    df['user_id'] = df['user'].map(user_ids)\n",
        "\n",
        "    return df, user_ids, scaler"
      ],
      "metadata": {
        "id": "XBJci2VLVit5"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_sequences(df, user_ids, seq_length):\n",
        "    features = ['lat', 'lon', 'alt', 'hour_sin', 'hour_cos', 'day_sin', 'day_cos']\n",
        "    targets = ['lat', 'lon']\n",
        "\n",
        "    X, y, users = [], [], []\n",
        "    for user, group in df.groupby('user'):\n",
        "        user_data = group[features].values\n",
        "        user_targets = group[targets].values\n",
        "        user_id = user_ids[user]\n",
        "\n",
        "        for i in range(len(user_data) - seq_length):\n",
        "            X.append(user_data[i:i+seq_length])\n",
        "            y.append(user_targets[i+seq_length])\n",
        "            users.append(user_id)\n",
        "\n",
        "    return np.array(X), np.array(y), np.array(users)"
      ],
      "metadata": {
        "id": "kKkSBCAPWbDj"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_contrastive_pairs_optimized(X, users, num_negatives=2, n_jobs=4):\n",
        "    unique_users = np.unique(users)\n",
        "    user_indices = {u: np.where(users == u)[0] for u in unique_users}\n",
        "\n",
        "    # Предварительный расчет всех возможных негативных пользователей\n",
        "    user_pairs = {u: unique_users[unique_users != u] for u in unique_users}\n",
        "\n",
        "    def process_user(user):\n",
        "        indices = user_indices[user]\n",
        "        pairs = []\n",
        "        labels = []\n",
        "\n",
        "        # Позитивные пары (векторизовано)\n",
        "        if len(indices) > 1:\n",
        "            pos_pairs = np.column_stack([indices[:-1], indices[1:]])\n",
        "            pairs.extend(pos_pairs)\n",
        "            labels.extend([1] * len(pos_pairs))\n",
        "\n",
        "            # Негативные пары (оптимизированный выбор)\n",
        "            for i, anchor in enumerate(indices[:-1]):\n",
        "                neg_users = np.random.choice(\n",
        "                    user_pairs[user],\n",
        "                    size=num_negatives,\n",
        "                    replace=len(user_pairs[user]) >= num_negatives\n",
        "                )\n",
        "                for neg_user in neg_users:\n",
        "                    neg_idx = np.random.choice(user_indices[neg_user])\n",
        "                    pairs.append([anchor, neg_idx])\n",
        "                    labels.append(0)\n",
        "\n",
        "        return np.array(pairs), np.array(labels)\n",
        "\n",
        "    # Распараллеливание по пользователям\n",
        "    results = Parallel(n_jobs=n_jobs)(delayed(process_user)(u) for u in unique_users)\n",
        "\n",
        "    # Объединение результатов\n",
        "    all_pairs = np.vstack([res[0] for res in results])\n",
        "    all_labels = np.concatenate([res[1] for res in results])\n",
        "\n",
        "    return all_pairs, all_labels"
      ],
      "metadata": {
        "id": "cIzfUzJqWfoa"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ContrastiveModel(tf.keras.Model):\n",
        "    def __init__(self, num_users, embedding_dim, seq_length, num_features):\n",
        "        super().__init__()\n",
        "        self.embedding = Embedding(num_users, embedding_dim)\n",
        "        self.lstm = LSTM(32)\n",
        "        self.dense = Dense(embedding_dim, activation='tanh')\n",
        "\n",
        "    def call(self, inputs):\n",
        "        seq, user_id = inputs\n",
        "        user_emb = self.embedding(user_id)\n",
        "        seq_features = self.lstm(seq)\n",
        "        return self.dense(tf.concat([seq_features, user_emb], axis=-1))"
      ],
      "metadata": {
        "id": "4n4IX7qhWlOp"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df, user_ids, scaler = load_and_preprocess_data(DATA_PATH)\n",
        "X, y, users = create_sequences(df, user_ids, SEQ_LENGTH)\n",
        "\n",
        "# Генерация пар\n",
        "pairs, labels = generate_contrastive_pairs_optimized(  # <-- Использовать оптимизированную версию\n",
        "    X,\n",
        "    users,\n",
        "    num_negatives=1,  # Уменьшить число негативных примеров для ускорения\n",
        "    n_jobs=8          # Использовать 8 ядер CPU\n",
        ")\n",
        "anchor_data = pairs[:, 0].astype(int)\n",
        "pair_data = pairs[:, 1]"
      ],
      "metadata": {
        "id": "c-WrI-wVWvPF"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "import pickle\n",
        "import os\n",
        "\n",
        "SAVE_PATH = \"/content/drive/My Drive/Colab Notebooks/contrastive_results/\"\n",
        "os.makedirs(SAVE_PATH, exist_ok=True)\n",
        "\n",
        "with open(SAVE_PATH + 'contrastive_pairs_data.pkl', 'wb') as f:\n",
        "    pickle.dump({\n",
        "        'pairs': pairs,\n",
        "        'labels': labels,\n",
        "        'anchor_data': anchor_data,\n",
        "        'pair_data': pair_data\n",
        "    }, f)\n",
        "\n",
        "print(f\"Все данные сохранены в: {SAVE_PATH}contrastive_pairs_data.pkl\")\n",
        "print(f\"Размеры данных:\")\n",
        "print(f\"- pairs: {pairs.shape}\")\n",
        "print(f\"- labels: {labels.shape}\")\n",
        "print(f\"- anchor_data: {anchor_data.shape}\")\n",
        "print(f\"- pair_data: {pair_data.shape}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "urIQRKiolqWs",
        "outputId": "b1806a84-5ad5-4bbf-9788-64ae0dabd8dc"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Все данные сохранены в: /content/drive/My Drive/Colab Notebooks/contrastive_results/contrastive_pairs_data.pkl\n",
            "Размеры данных:\n",
            "- pairs: (3691652, 2)\n",
            "- labels: (3691652,)\n",
            "- anchor_data: (3691652,)\n",
            "- pair_data: (3691652,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "contrastive_model = ContrastiveModel(\n",
        "    num_users=len(user_ids),\n",
        "    embedding_dim=EMBEDDING_DIM,\n",
        "    seq_length=SEQ_LENGTH,\n",
        "    num_features=X.shape[-1]\n",
        ")\n",
        "\n",
        "contrastive_model.compile(\n",
        "    optimizer=Adam(0.001),\n",
        "    loss=ContrastiveLoss(),\n",
        "    metrics=['accuracy']\n",
        ")"
      ],
      "metadata": {
        "id": "BoN02dqShQoq"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "anchor_data = np.array(anchor_data)\n",
        "if anchor_data.ndim == 2:  # (batch, features), а нам нужно (batch, seq, features)\n",
        "    anchor_data = np.expand_dims(anchor_data, axis=1)\n"
      ],
      "metadata": {
        "id": "eJyNNw8Pi7ug"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"anchor_data.shape: {anchor_data.shape}\")\n",
        "print(anchor_data[:5])  # Первые 5 элементов\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E1Kd3732jVkp",
        "outputId": "c3f57d88-3caf-4db5-e133-a04d8073e1c0"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "anchor_data.shape: (3691652,)\n",
            "[0 1 2 3 4]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Форма входных данных для LSTM:\", X[anchor_data].shape)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J_UGd1fbjH0T",
        "outputId": "9fb4ffdc-c213-4282-db67-e74111633cb0"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Форма входных данных для LSTM: (3691650, 10, 7)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "contrastive_model.fit(\n",
        "    [X[anchor_data], users[anchor_data]],\n",
        "    labels,\n",
        "    epochs=5,\n",
        "    batch_size=BATCH_SIZE\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 367
        },
        "id": "UGXgLvgVha8U",
        "outputId": "68da2c8c-c3c9-4cac-edb9-dd92b52a0bd0"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "Data cardinality is ambiguous. Make sure all arrays contain the same number of samples.'x' sizes: 3691650, 3691650\n'y' sizes: 3691652\n",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-20-78aecf4d9c57>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m contrastive_model.fit(\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0;34m[\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0manchor_data\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0musers\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0manchor_data\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mBATCH_SIZE\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras/src/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    120\u001b[0m             \u001b[0;31m# To get the full stack trace, call:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m             \u001b[0;31m# `keras.config.disable_traceback_filtering()`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 122\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    123\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m             \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras/src/trainers/data_adapters/data_adapter_utils.py\u001b[0m in \u001b[0;36mcheck_data_cardinality\u001b[0;34m(data)\u001b[0m\n\u001b[1;32m    113\u001b[0m             )\n\u001b[1;32m    114\u001b[0m             \u001b[0mmsg\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34mf\"'{label}' sizes: {sizes}\\n\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 115\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    116\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Data cardinality is ambiguous. Make sure all arrays contain the same number of samples.'x' sizes: 3691650, 3691650\n'y' sizes: 3691652\n"
          ]
        }
      ]
    }
  ]
}